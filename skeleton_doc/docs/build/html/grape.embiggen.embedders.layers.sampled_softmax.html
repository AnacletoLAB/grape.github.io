<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>grape.embiggen.embedders.layers.sampled_softmax package &mdash; GraPE 1.0.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> GraPE
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">grape.embiggen.embedders.layers.sampled_softmax package</a><ul>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax module</a></li>
<li><a class="reference internal" href="#module-grape.embiggen.embedders.layers.sampled_softmax">Module contents</a></li>
</ul>
</li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">GraPE</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>grape.embiggen.embedders.layers.sampled_softmax package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/grape.embiggen.embedders.layers.sampled_softmax.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="grape-embiggen-embedders-layers-sampled-softmax-package">
<h1>grape.embiggen.embedders.layers.sampled_softmax package<a class="headerlink" href="#grape-embiggen-embedders-layers-sampled-softmax-package" title="Permalink to this headline"></a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="module-grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax">
<span id="grape-embiggen-embedders-layers-sampled-softmax-sampledsoftmax-module"></span><h2>grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax module<a class="headerlink" href="#module-grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax" title="Permalink to this headline"></a></h2>
<p>Layer for executing Sampled Softmax in Keras models.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.activity_regularizer">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">activity_regularizer</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.activity_regularizer" title="Permalink to this definition"></a></dt>
<dd><p>Optional regularizer function for the output of this layer.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.compute_dtype">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">compute_dtype</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.compute_dtype" title="Permalink to this definition"></a></dt>
<dd><p>The dtype of the layer’s computations.</p>
<p>This is equivalent to <cite>Layer.dtype_policy.compute_dtype</cite>. Unless
mixed precision is used, this is the same as <cite>Layer.dtype</cite>, the dtype of
the weights.</p>
<p>Layers automatically cast their inputs to the compute dtype, which causes
computations and the output to be in the compute dtype as well. This is done
by the base Layer class in <cite>Layer.__call__</cite>, so you do not have to insert
these casts if implementing your own layer.</p>
<p>Layers often perform certain internal computations in higher precision when
<cite>compute_dtype</cite> is float16 or bfloat16 for numeric stability. The output
will still typically be float16 or bfloat16 in such cases.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The layer’s compute dtype.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.dtype">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">dtype</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.dtype" title="Permalink to this definition"></a></dt>
<dd><p>The dtype of the layer weights.</p>
<p>This is equivalent to <cite>Layer.dtype_policy.variable_dtype</cite>. Unless
mixed precision is used, this is the same as <cite>Layer.compute_dtype</cite>, the
dtype of the layer’s computations.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.dtype_policy">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">dtype_policy</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.dtype_policy" title="Permalink to this definition"></a></dt>
<dd><p>The dtype policy associated with this layer.</p>
<p>This is an instance of a <cite>tf.keras.mixed_precision.Policy</cite>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.dynamic">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">dynamic</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.dynamic" title="Permalink to this definition"></a></dt>
<dd><p>Whether the layer is dynamic (eager-only); set in the constructor.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.inbound_nodes">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">inbound_nodes</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.inbound_nodes" title="Permalink to this definition"></a></dt>
<dd><p>Deprecated, do NOT use! Only for compatibility with external Keras.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.input">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">input</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.input" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the input tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one input,
i.e. if it is connected to one incoming layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Input tensor or list of input tensors.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>RuntimeError</strong> – If called in Eager mode.</p></li>
<li><p><strong>AttributeError</strong> – If no inbound nodes are found.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.input_mask">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">input_mask</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.input_mask" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the input mask tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Input mask tensor (potentially None) or list of input
mask tensors.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>AttributeError</strong> – if the layer is connected to</p></li>
<li><p><strong>more than one incoming layers.</strong> – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.input_shape">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">input_shape</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.input_shape" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the input shape(s) of a layer.</p>
<p>Only applicable if the layer has exactly one input,
i.e. if it is connected to one incoming layer, or if all inputs
have the same shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Input shape, as an integer shape tuple
(or list of shape tuples, one tuple per input tensor).</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>AttributeError</strong> – if the layer has no defined input_shape.</p></li>
<li><p><strong>RuntimeError</strong> – if called in Eager mode.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.input_spec">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">input_spec</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.input_spec" title="Permalink to this definition"></a></dt>
<dd><p><cite>InputSpec</cite> instance(s) describing the input format for this layer.</p>
<p>When you create a layer subclass, you can set <cite>self.input_spec</cite> to enable
the layer to run input compatibility checks when it is called.
Consider a <cite>Conv2D</cite> layer: it can only be called on a single input tensor
of rank 4. As such, you can set, in <cite>__init__()</cite>:</p>
<p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">self.input_spec</span> <span class="pre">=</span> <span class="pre">tf.keras.layers.InputSpec(ndim=4)</span>
<span class="pre">`</span></code></p>
<p>Now, if you try to call the layer on an input that isn’t rank 4
(for instance, an input of shape <cite>(2,)</cite>, it will raise a nicely-formatted
error:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">ValueError:</span> <span class="pre">Input</span> <span class="pre">0</span> <span class="pre">of</span> <span class="pre">layer</span> <span class="pre">conv2d</span> <span class="pre">is</span> <span class="pre">incompatible</span> <span class="pre">with</span> <span class="pre">the</span> <span class="pre">layer:</span>
<span class="pre">expected</span> <span class="pre">ndim=4,</span> <span class="pre">found</span> <span class="pre">ndim=1.</span> <span class="pre">Full</span> <span class="pre">shape</span> <span class="pre">received:</span> <span class="pre">[2]</span>
<span class="pre">`</span></code></p>
<p>Input checks that can be specified via <cite>input_spec</cite> include:
- Structure (e.g. a single input, a list of 2 inputs, etc)
- Shape
- Rank (ndim)
- Dtype</p>
<p>For more information, see <cite>tf.keras.layers.InputSpec</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <cite>tf.keras.layers.InputSpec</cite> instance, or nested structure thereof.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.losses">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">losses</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.losses" title="Permalink to this definition"></a></dt>
<dd><p>List of losses added using the <cite>add_loss()</cite> API.</p>
<p>Variable regularization tensors are created when this property is accessed,
so it is eager safe: accessing <cite>losses</cite> under a <cite>tf.GradientTape</cite> will
propagate gradients back to the corresponding variables.</p>
<p>Examples:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">MyLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="gp">... </span>  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">inputs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">MyLayer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l</span><span class="o">.</span><span class="n">losses</span>
<span class="go">[1.0]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Activity regularization.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="s1">&#39;ones&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">d</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Weight regularization.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">kernel</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">losses</span>
<span class="go">[&lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt;]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.metrics">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">metrics</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.metrics" title="Permalink to this definition"></a></dt>
<dd><p>List of metrics added using the <cite>add_metric()</cite> API.</p>
<p>Example:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">d</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;max&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_min</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">metrics</span><span class="p">]</span>
<span class="go">[&#39;max&#39;, &#39;min&#39;]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of <cite>Metric</cite> objects.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.name">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">name</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.name" title="Permalink to this definition"></a></dt>
<dd><p>Name of the layer (string), set in the constructor.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.name_scope">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">name_scope</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.name_scope" title="Permalink to this definition"></a></dt>
<dd><p>Returns a <cite>tf.name_scope</cite> instance for this class.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.non_trainable_variables">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">non_trainable_variables</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.non_trainable_variables" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.non_trainable_weights">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">non_trainable_weights</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.non_trainable_weights" title="Permalink to this definition"></a></dt>
<dd><p>List of all non-trainable weights tracked by this layer.</p>
<p>Non-trainable weights are <em>not</em> updated during training. They are expected
to be updated manually in <cite>call()</cite>.</p>
<p>Note: This will not track the weights of nested <cite>tf.Modules</cite> that are not
themselves Keras layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of non-trainable variables.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.outbound_nodes">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">outbound_nodes</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.outbound_nodes" title="Permalink to this definition"></a></dt>
<dd><p>Deprecated, do NOT use! Only for compatibility with external Keras.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.output">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">output</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.output" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the output tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one output,
i.e. if it is connected to one incoming layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Output tensor or list of output tensors.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>AttributeError</strong> – if the layer is connected to more than one incoming
    layers.</p></li>
<li><p><strong>RuntimeError</strong> – if called in Eager mode.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.output_mask">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">output_mask</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.output_mask" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the output mask tensor(s) of a layer.</p>
<p>Only applicable if the layer has exactly one inbound node,
i.e. if it is connected to one incoming layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Output mask tensor (potentially None) or list of output
mask tensors.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>AttributeError</strong> – if the layer is connected to</p></li>
<li><p><strong>more than one incoming layers.</strong> – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.output_shape">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">output_shape</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.output_shape" title="Permalink to this definition"></a></dt>
<dd><p>Retrieves the output shape(s) of a layer.</p>
<p>Only applicable if the layer has one output,
or if all outputs have the same shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Output shape, as an integer shape tuple
(or list of shape tuples, one tuple per output tensor).</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>AttributeError</strong> – if the layer has no defined output shape.</p></li>
<li><p><strong>RuntimeError</strong> – if called in Eager mode.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.stateful">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">stateful</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.stateful" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.submodules">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">submodules</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.submodules" title="Permalink to this definition"></a></dt>
<dd><p>Sequence of all sub-modules.</p>
<p>Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on).</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Module</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Module</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Module</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">c</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">submodules</span><span class="p">)</span> <span class="o">==</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">submodules</span><span class="p">)</span> <span class="o">==</span> <span class="p">[</span><span class="n">c</span><span class="p">]</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">submodules</span><span class="p">)</span> <span class="o">==</span> <span class="p">[]</span>
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A sequence of all submodules.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.supports_masking">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">supports_masking</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.supports_masking" title="Permalink to this definition"></a></dt>
<dd><p>Whether this layer supports computing a mask using <cite>compute_mask</cite>.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.trainable">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">trainable</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.trainable" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.trainable_variables">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">trainable_variables</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.trainable_variables" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.trainable_weights">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">trainable_weights</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.trainable_weights" title="Permalink to this definition"></a></dt>
<dd><p>List of all trainable weights tracked by this layer.</p>
<p>Trainable weights are updated via gradient descent during training.</p>
<p>Note: This will not track the weights of nested <cite>tf.Modules</cite> that are not
themselves Keras layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of trainable variables.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.updates">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">updates</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.updates" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.variable_dtype">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">variable_dtype</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.variable_dtype" title="Permalink to this definition"></a></dt>
<dd><p>Alias of <cite>Layer.dtype</cite>, the dtype of the weights.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.variables">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">variables</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.variables" title="Permalink to this definition"></a></dt>
<dd><p>Returns the list of all layer variables/weights.</p>
<p>Alias of <cite>self.weights</cite>.</p>
<p>Note: This will not track the weights of nested <cite>tf.Modules</cite> that are not
themselves Keras layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of variables.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.weights">
<span class="sig-prename descclassname"><span class="pre">grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.</span></span><span class="sig-name descname"><span class="pre">weights</span></span><a class="headerlink" href="#grape.embiggen.embedders.layers.sampled_softmax.SampledSoftmax.weights" title="Permalink to this definition"></a></dt>
<dd><p>Returns the list of all layer variables/weights.</p>
<p>Note: This will not track the weights of nested <cite>tf.Modules</cite> that are not
themselves Keras layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of variables.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-grape.embiggen.embedders.layers.sampled_softmax">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-grape.embiggen.embedders.layers.sampled_softmax" title="Permalink to this headline"></a></h2>
<p>Layer for executing Sampled Softmax in Keras models.</p>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Cappelleti Luca, Fontana Tommaso.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>